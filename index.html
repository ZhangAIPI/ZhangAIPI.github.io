<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zeliang's Homepage</title>
  
  <meta name="author" content="Zeliang Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/website_icon.png">
</head>
 
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td halign="center">
          <p align="center">
              <font size="6">Zeliang Zhang</font>
          </p>
      </td>
    </tr>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <!-- <p style="text-align:center">
                <name>Chao Huang</name>
              </p> -->
              <p>
                I am a  PhD student in the Department of Computer Science at the University of Rochester, advised byÂ <a href="https://www.cs.rochester.edu/~cxu22/">Prof. Chenliang Xu</a>.  I received my B.Eng. from CS Department, Huazhong University of Science and Technology in 2022. In my undergrad, I worked with <a href="https://scholar.google.com/citations?user=YTQnGJsAAAAJ">Prof. He</a> and <a href="https://xiaosenwang.com/">Mr. Wang</a> at HUST on adversarial machine learning. I also work closely with <a href="https://scholar.google.com/citations?user=J9FPMToAAAAJ">Prof. Peng</a> at PKU on gradient estimation (Zero-Order optimization), and <a href="https://scholar.google.com/citations?user=C83b8ncAAAAJ&hl=en&oi=ao">Prof. Liu</a> at RPI/Columbia on high-performance quantum and tensor computation. I am good at playing Erhu and familiar with Violin. Welcome to reach out to chat:)
                <br>
                <br>
                Currently, I mainly work on efficient and reliable AI for my Ph.D degree. 
                <br> 
                ðŸ˜€ <Strong>I am finding friends working on the next-generation deep learning framework, which enjoys better efficiency, generalization, and robustness. If you are interested, please reach out to me at hust0426 AT gmail.com.</Strong>
              </p>
              <p style="text-align:center">
                <a href="mailto:zzh136@ur.rochester.edu">Email</a> &nbsp/&nbsp
                <a href="data/Zeliang_cv.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=7nLfsSgAAAAJ&hl=en">Google Scholar</a>  &nbsp/&nbsp
                <a href="https://space.bilibili.com/36945397?spm_id_from=333.1007.0.0">bilibili</a>  &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/wei-guan-de-pang-ci-i">Zhihu</a>  &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/ZhangAIPI">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/me.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
          </td>
        </tr>
      </tbody></table>
      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
      </tbody></table> -->
      <!-- <ul>
        <li><strong>[March 2023]</strong> I will be joining Meta Reality Labs Research Pitt this summer for internship! </li>
        <li><strong>[March 2023]</strong> I will be joining Meta Reality Labs Research Pitt this summer for internship! </li>
      </ul> -->
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <!-- <colgroup>
            <col width="15%">
            <col width="80%">
        </colgroup> -->
        <tbody>
          <tr>
          <td valign="top" align="center"><strong>[1/2024]</strong></td>
          <td>I will work as an Erhu performer at the Traditional Chinese Ensemble Group of Rochester.</td>
          </tr>
      <tr>
        <td valign="top" align="center"><strong>[10/2021]</strong></td>
        <td>I will work as a research intern in the machine learning for sustainability (MLSS) group of MSRA, Beijing.
        </td>
      </tr>
        </tbody>
    </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<be>(* indicates the equal contribution with random author order.)<be>	


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/vidLLM.png" alt="vidLLM" width="210" height="160">
      </td>
      <td width="75%" valign="middle">
          <a href="https://arxiv.org/abs/2312.17432"><papertitle>Video Understanding with Large Language Models: A Survey</papertitle></a>
        <br>
        Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, <Strong>Zeliang Zhang</Strong>, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu.
        <br>
        <em>Technical Report</em>, 2023
        <br>
        <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
        <p>This survey provides a detailed overview of the recent advancements in video understanding harnessing the power of LLMs.</p>
        <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
          <br>
          <em>CVPR Sight and Sound Workshop</em>, 2022 -->
          <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
      </td>
    </tr>


	        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pipeline.png" alt="pls_subclass" width="210" height="160">
            </td>
            <td width="75%" valign="middle">
              Multiple Unknown Biased Subclass Discovery in Latent Space
              <br>
              <strong>Zeliang Zhang*</strong>, Mingqian Feng*, Zhiheng Li, Chenliang Xu.
              <br>
              <em>Under review</em>, 2023
              <br>
              <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
              <p>We propose a novel method, namely DII (decomposition, identification, and interpretation), to debug the multi-bias of the models.</p>
              <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
                <br>
                <em>CVPR Sight and Sound Workshop</em>, 2022 -->
                <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/alr_opt.png" alt="alr_cnn" width="210" height="160">
            </td>
            <td width="75%" valign="middle">
              Approximated Likelihood Ratio Method for Neural Network Training
              <br>
              <strong>Zeliang Zhang*</strong>, Jinyang Jiang*, Zhuo Liu*, Yijie Peng, Chenliang Xu.
              <br>
              <em>Under review</em>, 2023
              <br>
              <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
              <p>We propose an approximated likelihood ratio method for gradient estimation without relimant on chain rule.</p>
              <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
                <br>
                <em>CVPR Sight and Sound Workshop</em>, 2022 -->
                <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gradient_estimation_CNN.png" alt="lr_cnn" width="210" height="160">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2305.08960.pdf"><papertitle>One Forward is Enough for Training Neural Networks via the Likelihood Ratio Method</papertitle></a>
              <br>
              Jinyang Jiang*, <strong>Zeliang Zhang*</strong>, Chenliang Xu, Zhaofei Yu, Yijie Peng.
              <br>
              <em>ICLR</em>, 2024
              <br>
              <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
              <p>We explore the potential of Likelihood ratio method for gradient estimation and train multi-architectures of NN without back-propagation.</p>
              <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
                <br>
                <em>CVPR Sight and Sound Workshop</em>, 2022 -->
                <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tricks_paper.png" alt="point_denoise" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/pdf?id=L6CgvBarc4"><papertitle></papertitle>Bag of Tricks to Boost the Adversarial Transferability</papertitle></a>
              <br>
              <strong>Zeliang Zhang</strong>, Rongyi Zhu, Wei Yao, Xiaosen Wang, Chenliang Xu
              <br>
              <em>Technical  Report</em>, 2024
              <p>We propose a bag of novel tricks to boost the adversarial transferability among different models.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/av_robustness.png" alt="point_denoise" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
              Revisit Audio-Visual Adversarial Robustness from Temporal and Modality Correlation Perspectives
              <br>
              <strong>Zeliang Zhang*</strong>, Susan Liang*, Daiki Shimada*, Chenliang Xu
              <br>
              <em>Under review</em>, 2023
              <p>We revisit the audio-visual model robustness from the temporal and modality correlation perspectives, propose a strong adversarial attack as the benchmark, and use bag of tricks to improve the adversarial training for audio-visual robustness.</p>
            </td>
          </tr>

          


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/text_paper.png" alt="extreme_compression" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
            Random Smooth-based Certified Defense against Text Adversarial Attack
              <br>
              <strong>Zeliang Zhang</strong>, Wei Yao, Susan Liang, Chenliang Xu
              <br>
              <em>EACL Findings</em>, 2024
              <p> We propose to treat the word substitution as a continuous perturbation on the word embedding representation for better robustness. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sit_paper.png" alt="extreme_compression" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2309.14700.pdf"><papertitle>Structure Invariant Transformation for better Adversarial Transferability</papertitle></a>
              <br>
              Xiaosen Wang, <strong>Zeliang Zhang</strong>, Jianping Zhang
              <br>
              <em>ICCV</em>, 2023
              <p> We propose a novel input transformation based attack, called Structure Invariant Transformation (SIA), which applies a random image transformation onto each image block to craft a set of diverse images for gradient calculation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tensor_train.png" alt="extreme_compression" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
              High-performance Tensor-Train Primitives Using GPU Tensor Cores
              <br>
              Xiao-Yang Liu, Hao Hong, <strong>Zeliang Zhang</strong>, Weiqing Tong, Xiaodong Wang, Anwar Walid 
              <br>
              <em>Under review</em>, 2023
              <p> We present high-performance tensor-train primitives using GPU tensor cores and demonstrate three applications.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dhf_paper.png" alt="extreme_compression" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2304.10136.pdf"><papertitle>Diversifying the High-level Features for better Adversarial Transferability</papertitle></a>
              <br>
              Zhiyuan Wang*, <strong>Zeliang Zhang*</strong>, Siyuan Liang, Xiaosen Wang
              <br>
              <em>BMVC</em>, 2023, oral
              <p> We propose diversifying the high-level features (DHF) for more transferable adversarial examples.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fair.png" alt="extreme_compression" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2207.04581.pdf"><papertitle>How Robust is your Fair Model? Exploring the Robustness of Diverse Fairness Strategies</papertitle></a>
              <br>
              Edward Small, Wei Shao, <strong>Zeliang Zhang</strong>, Peihan Liu, Jeffrey Chan, Kacper Sokol, Flora Salim
              <br>
              <em>Technical Report</em>, 2023
              <p> We quantitatively evaluate the robustness of fairness optimization strategies.  </p>
            </td>
          </tr>

	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/quantum_paper.png" alt="av_nerf" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
		<a href="https://openreview.net/pdf?id=T5ArxPU3Oq"><papertitle>Classical Simulation of Quantum Circuits: Parallel Environments and Benchmark</papertitle></a>
              <br>
              Xiao-Yang Liu, <strong>Zeliang Zhang</strong>
              <br>
              <em>NeurIPS dataset and benchmark track</em>, 2023
              <p>We develop a dozen of massively parallel environments to simulate quantum circuits. We open-source our parallel gym environments and benchmarks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tensor_paper.png" alt="lr_cnn" width="210" height="160">
            </td>
            <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/9987675"><papertitle>High-Performance Tensor Learning Primitives Using GPU Tensor Cores</papertitle></a>
              <br>
              Xiao-Yang Liu*, <strong>Zeliang Zhang*</strong>, Zhiyuan Wang, Han Lu, Xiaodong Wang, Anwar Walid.
              <br>
              <em>IEEE Transaction on Computers</em>, 2022
              <br>
              <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
              <p>We propose novel hardware-oriented optimization strategies for tensor learning primitives on GPU tensor cores.</p>
              <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
                <br>
                <em>CVPR Sight and Sound Workshop</em>, 2022 -->
                <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ta_paper.png" alt="lr_cnn" width="210" height="160">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2112.06569.pdf"><papertitle>Triangle Attack: A Query-efficient Decision-based Adversarial Attack</papertitle></a>
              <br>
              Xiaosen Wang, <strong>Zeliang Zhang</strong>, Kangheng Tong, Dihong Gong, Kun He, Zhifeng Li, Wei Liu
              <br>
              <em>ECCV</em>, 2022
              <br>
              <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
              <p>We propose a novel Triangle Attack (TA) to optimize the perturbation by utilizing the geometric information that the longer side is always opposite the larger angle in any triangle.</p>
              <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
                <br>
                <em>CVPR Sight and Sound Workshop</em>, 2022 -->
                <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/case_paper.png" alt="lr_cnn" width="210" height="160">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2102.04450.pdf"><papertitle>Noise Optimization for Artificial Neural Networks</papertitle></a>
              <br>
              Li Xiao, <strong>Zeliang Zhang</strong>, Yijie Peng
              <br>
              <em>CASE</em>, 2022
              <br>
              <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
              <p>We propose a new technique to compute the pathwise stochastic gradient estimate with respect to the standard deviation of the Gaussian noise added to each neuron of the ANN.</p>
              <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
                <br>
                <em>CVPR Sight and Sound Workshop</em>, 2022 -->
                <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
            </td>
          </tr>

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Education</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td tyle="padding:20px;width:25%;vertical-align:middle"><img src="images/uofr-logo-shield.png" width="120" ></td>
            <td width="75%" valign="center">
              <strong>University of Rochester </strong>, NY, USA
              <br>
              Ph.D. in Computer Science
              <br>
              Sep. 2022 - Present
              <br> 
              Advisor: <a href="https://www.cs.rochester.edu/~cxu22/p/index.html">Chenliang Xu</a>
            </td>
          </tr>
          <tr>
            <td tyle="padding:20px;width:25%;vertical-align:middle"><img src="images/hust.jpg"  width="120"></td>
            <td width="75%" valign="center">
              <strong>Huazhong University of Science and Technology</strong>, Wuhan, China
              <br>
              B.Eng in Computer Science and Technology
              <br>
              Sept. 2018 - Jun. 2022
            </td>
          </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/msra.png" width="120" ></td>
            <td width="75%" valign="center">
              <strong>Microsoft Research Asia </strong>, Beijing, China
              <br>
              Research Intern
              <br>
              Oct. 2021 - Jun. 2022
              <br> 
              Advisor: <a href="https://jialrs.github.io/home/">Jia Zhang</a> and Xinran Wei
              <br>
              Work on high-performance computation of DFT, which is important/bottleneck in material design using AI. 
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cmb.png" width="120" ></td>
            <td width="75%" valign="center">
              <strong>Columbia University </strong>, NYC, US
              <br>
              Research Assistant
              <br>
              Feb. 2020 - Dec. 2022
              <br> 
              Advisor: <a href=https://scholar.google.com/citations?user=C83b8ncAAAAJ&hl=en&oi=ao>Xiao-Yang Liu</a>
              <br>
              Work on high-performance tensor computation using GPUs and publish two workshop papers on top conference, namely the <a href=https://tensorworkshop.github.io/2020/accepted_papers/4-Trillion-Tensor-Trillion-Scale%20CP%20Tensor%20Decomposition.pdf>"Trillion-Tensor: Trillion-Scale CP Tensor Decomposition"</a> at IJCAI 2020 TNRML workshop and <a href=https://tensorworkshop.github.io/NeurIPS2020/accepted_papers/NIPS_2020_Workshop_Zeliang__Copy_%20(6).pdf>"Parallel TTr1-Tensor: Randomized Compression-based Scheme for Tensor Train Rank-1 Decomposition"</a> at NIPS 2020 QTNML workshop.
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/erl.jpg" width="120" ></td>
            <td width="75%" valign="center">
              <a href="https://github.com/AI4Finance-Foundation/ElegantRL"><papertitle>Elegant RL </papertitle></a> (~ 3k stars! &#128640)
              Develop the RL-Hamiltonian algorithm to stablize the RL training and publish it as a <a href="https://tensorworkshop.github.io/NeurIPS2021/accepted_papers/DHN_Variational_RL_27.pdf">poster</a> in NIPS 2021 QTNML workshop.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/ta_overview.png" width="120" ></td>
            <td width="75%" valign="center">
              <a href="https://github.com/Trustworthy-AI-Group/TransferAttack"><papertitle>TransferAttack </papertitle></a> 
              One of the main contributors. TransferAttack is a pytorch framework to boost the adversarial transferability for image classification.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/quantum_circuit.png" width="120" ></td>
            <td width="75%" valign="center">
              <a href="https://github.com/YangletLiu/RL4QuantumCircuits"><papertitle>RL for Quantum Circuits </papertitle></a> 
              One of the main contributors. Open-source benchmark and environments for the classical simulation of quantum circuits.  
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                The template is based on <a href="http://jonbarron.info/">Jon Barron</a>'s website.
              </p>
              <a style="vertical-align:middle" href="https://clustrmaps.com/site/1aokh"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=1Hy1olEvEwHbSO1bS0j16i-wjqkfkVfzM6ADMGK3X7k&cl=ffffff" /></a>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
